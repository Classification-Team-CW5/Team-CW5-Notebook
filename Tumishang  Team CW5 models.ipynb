{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team CW5 EDSA - Climate Change Belief Analysis 2022\n",
    "\n",
    "Predict an individualâ€™s belief in climate change based on historical tweet data\n",
    "\n",
    "###### Members\n",
    "1. Tumishang Mankoe\n",
    "2. Thembani\n",
    "3. Bongo\n",
    "4. Lerato\n",
    "5. Patrick\n",
    "6. Kamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 14:22:16 +02:00)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "time: 3min 53s (started: 2022-06-23 14:41:29 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#!pip install comet-ml\n",
    "!conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 14:37:18 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# Import comet_ml at the top of your file\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/tumishang/global-warming-climate-change-sentiment-analysis/79522ee45a6e4233bf9f8f36d6d8b0c4\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-info          : 1\n",
      "COMET INFO:     conda-specification : 1\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     notebook            : 1\n",
      "COMET INFO:     source_code         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Couldn't find a Git repository in 'C:\\\\Users\\\\tumis\\\\Downloads\\\\EDSA Data Science\\\\Machine Learning\\\\Advanced Classification\\\\Global_warming-Climate_Change_Sentiment_Analysis_ZM3-main\\\\Global_warming-Climate_Change_Sentiment_Analysis_ZM3-main' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/tumishang/global-warming-climate-change-sentiment-analysis/83a4c55ae1f44ed5abfea690ade303b7\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.52 s (started: 2022-06-23 14:37:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"eqzMhfz18Q65B29PZlRqYSCK9\",\n",
    "    project_name=\"global-warming-climate-change-sentiment-analysis\",\n",
    "    workspace=\"tumishang\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "###### Using Twitter to measure the impact of climate change:\n",
    "As the climate crisis intensifies and natural disasters become more frequent and powerful, scientists are increasingly turning to social media as a way to assess the damage and impact on a more localized scale. In our case, Twitter was useful given the geographical reach of Twitter as well as the volume and location-specific nature of tweets. The platform can be used to track how individuals feel about climate change and how they view climate change.\n",
    "\n",
    "Social media encourages greater knowledge of climate change, mobilization of climate change activists, space for discussing the issue with others, and online discussions that frame climate change as a negative for society. Social media, however, does provide space for framing climate change skeptically and activating those with a skeptical perspective of climate change.\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://media.tenor.com/images/47d160eabb0927ed23827ab099ee83c3/tenor.gif\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=500px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "We aim to explore machine learning as a method to assist us in identifying whether or not a person believes in climate change and could possibly be converted to a new customer based on their tweets. To do so, we will develop a ML model that is able to classify textual passage as relevant to climate change adaptation. To produce such a model, we first select an appropriate corpus of documents for training that has been annotated. Then, we pre-process and clean the documents, transform them to extract appropriate features, select a ML model, train it, and evaluate its performance. Model evaluation is done both by comparing model predictions against a human panel at block level and comparing model performance against data that have been annotated but not used for training using cross-fold validation. Once a satisfactory performance of the model has been achieved, we interpret the patterns learned and apply them for further decision-making in a climate change adaptation context.\n",
    "\n",
    "<div align=\"center\" style=\"width: 500px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://en.reset.org/files/imagecache/sc_832x468/2018/02/27/planet_earth.jpg\"\n",
    "     alt=\"Dummy image 1\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=500px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\tumis\\appdata\\roaming\\python\\python39\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\tumis\\appdata\\roaming\\python\\python39\\site-packages (from imblearn) (0.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\tumis\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\tumis\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\tumis\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tumis\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\tumis\\appdata\\roaming\\python\\python39\\site-packages (from imbalanced-learn->imblearn) (1.1.1)time: 10.6 s (started: 2022-06-23 14:37:33 +02:00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install imblearn --user\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2022-06-23 14:46:59 +02:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tumis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tumis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download(['stopwords','punkt'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.58 s (started: 2022-06-23 14:47:05 +02:00)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/Classification-Team-CW5/Classification-Data/main/train.csv\")\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/Classification-Team-CW5/Classification-Data/main/test_with_no_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"EDA\"></a>\n",
    "# Exploratory Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the train and test data in order to do preprocessing on both datasets. This is essential to test the models being built. This step will also be useful for implementing the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function which will preprocess all of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 14:47:07 +02:00)\n"
     ]
    }
   ],
   "source": [
    "def tweet_preprocessing(tweet):\n",
    "    \n",
    "    '''\n",
    "    This functions cleans tweets from line breaks, URLs, numbers, etc.\n",
    "    '''\n",
    "    \n",
    "    tweet = tweet.lower() #to lower case\n",
    "    tweet = tweet.replace('\\n', ' ') # remove line breaks\n",
    "    tweet = tweet.replace('\\@(\\w*)', '') # remove mentions\n",
    "    tweet = re.sub(r\"\\bhttps://t.co/\\w+\", '', tweet) # remove URLs\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet) # remove numbers\n",
    "    tweet = re.sub(r'\\#', '', tweet) # remove hashtags. To remove full hashtag: '\\#(\\w*)'\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet) # removes numbers?\n",
    "    tweet = re.sub(' +', ' ', tweet) # remove 1+ spaces\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we create a function for preprocessing we must split the data into labels and features (X and y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.61 s (started: 2022-06-23 14:47:09 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the labels and features\n",
    "train['processed'] = train['message'].apply(tweet_preprocessing)\n",
    "X = train['processed'].values\n",
    "y = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 s (started: 2022-06-23 14:47:10 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# preprocess testing data by applying our function\n",
    "test['processed'] = test['message'].apply(tweet_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature\"></a>\n",
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2022-06-23 14:47:12 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the labels and fetures into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 703 ms (started: 2022-06-23 14:47:13 +02:00)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "mnb = Pipeline([('Count',CountVectorizer()),('classify',MultinomialNB())])\n",
    "#fitting the model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "#apply model on test data\n",
    "y_pred_mnb = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.28      0.42       130\n",
      "           0       0.68      0.22      0.33       235\n",
      "           1       0.69      0.92      0.79       853\n",
      "           2       0.78      0.69      0.73       364\n",
      "\n",
      "    accuracy                           0.71      1582\n",
      "   macro avg       0.75      0.53      0.57      1582\n",
      "weighted avg       0.72      0.71      0.68      1582\n",
      "\n",
      "time: 16 ms (started: 2022-06-23 14:47:18 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the '-1' and '0' class are poorly predicted when using unbalanced data. Once we implement resampling their f1-score increases for these model but only slightly. While at the same time the overall accuracy is slightly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modelling\"></a>\n",
    "# MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC and LinearSVC\n",
    "\n",
    "SVC Provides a best fit to catergorize our data this fit can be nonlinear, while a linearSVC provides a linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 14:49:54 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "svc = Pipeline([('Count',CountVectorizer()),('classify',SVC(max_iter=300,C=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2022-06-23 14:49:57 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#linearSVC\n",
    "linsvc = Pipeline([('Count',CountVectorizer()),('classify',LinearSVC(max_iter=300,C=1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Models the discrete probability distribution between classes and classifies based on the inflection point of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 14:50:01 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr = Pipeline([('Count',CountVectorizer()),('classify',LogisticRegression(max_iter=300))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "The KNN classifier assumes that all data points that are close together fall into the same class.K is the number of neighbours. So K=3 implies we will make our predictions based off f the 3 closest points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 14:50:04 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn = Pipeline([('Count',CountVectorizer()),('classify',KNeighborsClassifier(n_neighbors=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "\n",
    "The decision tree uses a tree-like model of decisions and their possible consequences including chance event outcomes, resource costs and utility.Starting from the decision itself (called a \"node\"), each branch of the decision tree represents a possible decision, outcome, or reaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 14:50:06 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "dt = Pipeline([('Count',CountVectorizer()),('classify',DecisionTreeClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Using the decision tree as a base estimator,each estimator is trained on a different bootstrap sample having the same size as the training set. At each node of the forest, features are sampled without replacement to increase randomization. Nodes are split to maximise information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 14:50:08 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "rf = Pipeline([('Count',CountVectorizer()),('classify',RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average weighted F1 score over 3 SVC models is 0.5244944329747319\n",
      "time: 25 s (started: 2022-06-23 14:50:12 +02:00)\n"
     ]
    }
   ],
   "source": [
    "num=3\n",
    "# SVC\n",
    "scores = cross_val_score(\n",
    "        svc, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' SVC models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average weighted F1 score over 3 LinearSVC models is 0.7186898909952019\n",
      "time: 7.09 s (started: 2022-06-23 14:50:37 +02:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#linearSVC\n",
    "scores = cross_val_score(\n",
    "        linsvc, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+ ' LinearSVC models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average weighted F1 score over 3 Logistic Regression models is 0.731476571299723\n",
      "time: 19.3 s (started: 2022-06-23 14:51:10 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "scores = cross_val_score(\n",
    "        lr, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' Logistic Regression models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average weighted F1 score over 3 KNN models is 0.52540597989338\n",
      "time: 13.2 s (started: 2022-06-23 14:51:37 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "scores = cross_val_score(\n",
    "        knn, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' KNN models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average weighted F1 score over 3 Decision Tree models is 0.6222277341051241\n",
      "time: 12.3 s (started: 2022-06-23 14:51:54 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "scores = cross_val_score(\n",
    "        dt, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' Decision Tree models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average weighted F1 score over 3 KNN models is 0.6707037567346896\n",
      "time: 1min 6s (started: 2022-06-23 14:52:14 +02:00)\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "scores = cross_val_score(\n",
    "        rf, X, y, cv=num, scoring='f1_weighted')\n",
    "print('The average weighted F1 score over '+str(num)+' KNN models is ' + str(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression Model and the LinearSVC model perform the best. The best performance for every model is found when resampling is not done. This could be because because upsampling the minority classes to the level of the majority class results in too much overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning parameters\n",
    "\n",
    "We take a look and see if we can improve our best 2 models: linearSVC and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 49.8 s (started: 2022-06-23 14:57:11 +02:00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "param_grid = {\n",
    "    'C'     : Cs\n",
    "    }\n",
    "grid_SVM = GridSearchCV(LogisticRegression(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid_SVM.fit(CountVectorizer().fit_transform(X), y)\n",
    "grid_SVM.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.1}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.4 s (started: 2022-06-23 14:59:33 +02:00)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C'     : Cs }\n",
    "grid_SVM = GridSearchCV(LinearSVC(), param_grid, scoring='f1_weighted', cv=3)\n",
    "grid_SVM.fit(CountVectorizer().fit_transform(X), y)\n",
    "grid_SVM.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "# Conclusion\n",
    "\n",
    "#### Model performance\n",
    "Several strategies we attempted to improve model performance, ranging from data processing techniques to clean the tweets, data balancing strategies, cross validation and grid search for the best values for model hyperparameters.\n",
    "\n",
    "On the whole, the models performed better on the uncleaned data. Data balancing strategies yielded little to no improvement in model performance. A few models that were tried resulted in overfitting.\n",
    "\n",
    "\n",
    "#### What else we can try\n",
    "Language models and the use of neural networks were two other strategies that we wanted to implement, to see how the performance of model improves with the use of a language model, and how a neural network performance.\n",
    "\n",
    "#### Business case value\n",
    "\n",
    "From the above analysis, the story that is emerging is fairly clear; the sentiment from the negative class of tweets is that of individuals who consider the science of climate change as being a deceit. Seeing that the debate has also become ideological, it would probably be best to tailor a message to this group that does not emphasize the environmental friendliness and sustainability aspects of the products and services, but rather a message that speaks to product features and price etc, would be the best approach when targeting this group.\n",
    "\n",
    "On the other hand, individuals from the positive class of tweets certainly believe in climate change, it is however not clear whether these individuals in their daily lives necessarily make decisions based on the environmental friendliness and sustainability of the products and services they purchase. Emphasizing a message of environmental friendliness and sustainability within this group, will not negatively impact how the products and services are received.\n",
    "\n",
    "\n",
    "Some organisations are mentioned in the tweets, many which share the same values and ideals when it comes to protecting the environment, who have a substantial membership and following on social media of individuals who share the same values and ideals. The formation of potential partnerships with these organisations could lead to brand exposure with individuals who in their daily lives make conscious decisions with regards to the products and services they purchase.\n",
    "\n",
    "We recommend that the latter strategy of pursuing partnerships with like minded organisations will yield the best results, in terms of finding a group of potential customers who share the same values and ideals, and would be likely to purchase your products and services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "# SUBMISSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final model, we build a stacking classifier to combine Logistic Regression, LinearSVC and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 15:16:16 +02:00)\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "       ('rf', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',RandomForestClassifier())])),\n",
    "         \n",
    "        ('lnsvc', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',LinearSVC(C=0.1))])),\n",
    "         \n",
    "        ('MNB',Pipeline([('Count',CountVectorizer()),('classify',MultinomialNB())])),\n",
    "    \n",
    "        ('lr', Pipeline([('Count',CountVectorizer(ngram_range=(1,2))),('classify',LogisticRegression(C=1))]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET ERROR: Failed to extract parameters from estimator\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\tumis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "COMET ERROR: Failed to extract parameters from estimator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('rf',\n",
       "                                Pipeline(steps=[('Count',\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              2))),\n",
       "                                                ('classify',\n",
       "                                                 RandomForestClassifier())])),\n",
       "                               ('lnsvc',\n",
       "                                Pipeline(steps=[('Count',\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              2))),\n",
       "                                                ('classify',\n",
       "                                                 LinearSVC(C=0.1))])),\n",
       "                               ('MNB',\n",
       "                                Pipeline(steps=[('Count', CountVectorizer()),\n",
       "                                                ('classify',\n",
       "                                                 MultinomialNB())])),\n",
       "                               ('lr',\n",
       "                                Pipeline(steps=[('Count',\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              2))),\n",
       "                                                ('classify',\n",
       "                                                 LogisticRegression(C=1))]))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9min 30s (started: 2022-06-23 16:46:08 +02:00)\n"
     ]
    }
   ],
   "source": [
    "clf = StackingClassifier(estimators=estimators)\n",
    "\n",
    "#fitting the model\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/tumishang/global-warming-climate-change-sentiment-analysis/83a4c55ae1f44ed5abfea690ade303b7\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     C                                  : 1\n",
      "COMET INFO:     MNB                                : Pipeline(steps=[('Count', CountVectorizer()), ('classify', MultinomialNB())])\n",
      "COMET INFO:     MNB__Count                         : CountVectorizer()\n",
      "COMET INFO:     MNB__Count__analyzer               : word\n",
      "COMET INFO:     MNB__Count__binary                 : False\n",
      "COMET INFO:     MNB__Count__decode_error           : strict\n",
      "COMET INFO:     MNB__Count__dtype                  : <class 'numpy.int64'>\n",
      "COMET INFO:     MNB__Count__encoding               : utf-8\n",
      "COMET INFO:     MNB__Count__input                  : content\n",
      "COMET INFO:     MNB__Count__lowercase              : True\n",
      "COMET INFO:     MNB__Count__max_df                 : 1.0\n",
      "COMET INFO:     MNB__Count__max_features           : 1\n",
      "COMET INFO:     MNB__Count__min_df                 : 1\n",
      "COMET INFO:     MNB__Count__ngram_range            : (1, 1)\n",
      "COMET INFO:     MNB__Count__preprocessor           : 1\n",
      "COMET INFO:     MNB__Count__stop_words             : 1\n",
      "COMET INFO:     MNB__Count__strip_accents          : 1\n",
      "COMET INFO:     MNB__Count__token_pattern          : (?u)\\b\\w\\w+\\b\n",
      "COMET INFO:     MNB__Count__tokenizer              : 1\n",
      "COMET INFO:     MNB__Count__vocabulary             : 1\n",
      "COMET INFO:     MNB__classify                      : MultinomialNB()\n",
      "COMET INFO:     MNB__classify__alpha               : 1.0\n",
      "COMET INFO:     MNB__classify__class_prior         : 1\n",
      "COMET INFO:     MNB__classify__fit_prior           : True\n",
      "COMET INFO:     MNB__memory                        : 1\n",
      "COMET INFO:     MNB__steps                         : [('Count', CountVectorizer()), ('classify', MultinomialNB())]\n",
      "COMET INFO:     MNB__verbose                       : False\n",
      "COMET INFO:     algorithm                          : auto\n",
      "COMET INFO:     alpha                              : 1.0\n",
      "COMET INFO:     bootstrap                          : True\n",
      "COMET INFO:     break_ties                         : False\n",
      "COMET INFO:     cache_size                         : 200\n",
      "COMET INFO:     ccp_alpha                          : 0.0\n",
      "COMET INFO:     class_prior                        : 1\n",
      "COMET INFO:     class_weight                       : 1\n",
      "COMET INFO:     coef0                              : 0.0\n",
      "COMET INFO:     criterion                          : gini\n",
      "COMET INFO:     cv                                 : 2\n",
      "COMET INFO:     decision_function_shape            : ovr\n",
      "COMET INFO:     degree                             : 3\n",
      "COMET INFO:     dual                               : False\n",
      "COMET INFO:     error_score                        : nan\n",
      "COMET INFO:     estimator                          : LinearSVC()\n",
      "COMET INFO:     estimator__C                       : 1.0\n",
      "COMET INFO:     estimator__class_weight            : 1\n",
      "COMET INFO:     estimator__dual                    : True\n",
      "COMET INFO:     estimator__fit_intercept           : True\n",
      "COMET INFO:     estimator__intercept_scaling       : 1\n",
      "COMET INFO:     estimator__l1_ratio                : 1\n",
      "COMET INFO:     estimator__loss                    : squared_hinge\n",
      "COMET INFO:     estimator__max_iter                : 1000\n",
      "COMET INFO:     estimator__multi_class             : ovr\n",
      "COMET INFO:     estimator__n_jobs                  : 1\n",
      "COMET INFO:     estimator__penalty                 : l2\n",
      "COMET INFO:     estimator__random_state            : 1\n",
      "COMET INFO:     estimator__solver                  : lbfgs\n",
      "COMET INFO:     estimator__tol                     : 0.0001\n",
      "COMET INFO:     estimator__verbose                 : 0\n",
      "COMET INFO:     estimator__warm_start              : False\n",
      "COMET INFO:     estimators                         : [('rf', Pipeline(steps=[('Count', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('classify', RandomForestClassifier())])), ('lnsvc', Pipeline(steps=[('Count', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('classify', LinearSVC(C=0.1))])), ('MNB', Pipeline(steps=[('Count', CountVectorizer()), ('classify', MultinomialNB())])), ('lr', Pipeline(steps=[('Count', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('classify', LogisticRegression(C=1))]))]\n",
      "COMET INFO:     final_estimator                    : 1\n",
      "COMET INFO:     fit_intercept                      : True\n",
      "COMET INFO:     fit_prior                          : True\n",
      "COMET INFO:     gamma                              : scale\n",
      "COMET INFO:     intercept_scaling                  : 1\n",
      "COMET INFO:     kernel                             : rbf\n",
      "COMET INFO:     l1_ratio                           : 1\n",
      "COMET INFO:     leaf_size                          : 30\n",
      "COMET INFO:     lnsvc                              : Pipeline(steps=[('Count', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('classify', LinearSVC(C=0.1))])\n",
      "COMET INFO:     lnsvc__Count                       : CountVectorizer(ngram_range=(1, 2))\n",
      "COMET INFO:     lnsvc__Count__analyzer             : word\n",
      "COMET INFO:     lnsvc__Count__binary               : False\n",
      "COMET INFO:     lnsvc__Count__decode_error         : strict\n",
      "COMET INFO:     lnsvc__Count__dtype                : <class 'numpy.int64'>\n",
      "COMET INFO:     lnsvc__Count__encoding             : utf-8\n",
      "COMET INFO:     lnsvc__Count__input                : content\n",
      "COMET INFO:     lnsvc__Count__lowercase            : True\n",
      "COMET INFO:     lnsvc__Count__max_df               : 1.0\n",
      "COMET INFO:     lnsvc__Count__max_features         : 1\n",
      "COMET INFO:     lnsvc__Count__min_df               : 1\n",
      "COMET INFO:     lnsvc__Count__ngram_range          : (1, 2)\n",
      "COMET INFO:     lnsvc__Count__preprocessor         : 1\n",
      "COMET INFO:     lnsvc__Count__stop_words           : 1\n",
      "COMET INFO:     lnsvc__Count__strip_accents        : 1\n",
      "COMET INFO:     lnsvc__Count__token_pattern        : (?u)\\b\\w\\w+\\b\n",
      "COMET INFO:     lnsvc__Count__tokenizer            : 1\n",
      "COMET INFO:     lnsvc__Count__vocabulary           : 1\n",
      "COMET INFO:     lnsvc__classify                    : LinearSVC(C=0.1)\n",
      "COMET INFO:     lnsvc__classify__C                 : 0.1\n",
      "COMET INFO:     lnsvc__classify__class_weight      : 1\n",
      "COMET INFO:     lnsvc__classify__dual              : True\n",
      "COMET INFO:     lnsvc__classify__fit_intercept     : True\n",
      "COMET INFO:     lnsvc__classify__intercept_scaling : 1\n",
      "COMET INFO:     lnsvc__classify__loss              : squared_hinge\n",
      "COMET INFO:     lnsvc__classify__max_iter          : 1000\n",
      "COMET INFO:     lnsvc__classify__multi_class       : ovr\n",
      "COMET INFO:     lnsvc__classify__penalty           : l2\n",
      "COMET INFO:     lnsvc__classify__random_state      : 1\n",
      "COMET INFO:     lnsvc__classify__tol               : 0.0001\n",
      "COMET INFO:     lnsvc__classify__verbose           : 0\n",
      "COMET INFO:     lnsvc__memory                      : 1\n",
      "COMET INFO:     lnsvc__steps                       : [('Count', CountVectorizer(ngram_range=(1, 2))), ('classify', LinearSVC(C=0.1))]\n",
      "COMET INFO:     lnsvc__verbose                     : False\n",
      "COMET INFO:     loss                               : squared_hinge\n",
      "COMET INFO:     lr                                 : Pipeline(steps=[('Count', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('classify', LogisticRegression(C=1))])\n",
      "COMET INFO:     lr__Count                          : CountVectorizer(ngram_range=(1, 2))\n",
      "COMET INFO:     lr__Count__analyzer                : word\n",
      "COMET INFO:     lr__Count__binary                  : False\n",
      "COMET INFO:     lr__Count__decode_error            : strict\n",
      "COMET INFO:     lr__Count__dtype                   : <class 'numpy.int64'>\n",
      "COMET INFO:     lr__Count__encoding                : utf-8\n",
      "COMET INFO:     lr__Count__input                   : content\n",
      "COMET INFO:     lr__Count__lowercase               : True\n",
      "COMET INFO:     lr__Count__max_df                  : 1.0\n",
      "COMET INFO:     lr__Count__max_features            : 1\n",
      "COMET INFO:     lr__Count__min_df                  : 1\n",
      "COMET INFO:     lr__Count__ngram_range             : (1, 2)\n",
      "COMET INFO:     lr__Count__preprocessor            : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO:     lr__Count__stop_words              : 1\n",
      "COMET INFO:     lr__Count__strip_accents           : 1\n",
      "COMET INFO:     lr__Count__token_pattern           : (?u)\\b\\w\\w+\\b\n",
      "COMET INFO:     lr__Count__tokenizer               : 1\n",
      "COMET INFO:     lr__Count__vocabulary              : 1\n",
      "COMET INFO:     lr__classify                       : LogisticRegression(C=1)\n",
      "COMET INFO:     lr__classify__C                    : 1\n",
      "COMET INFO:     lr__classify__class_weight         : 1\n",
      "COMET INFO:     lr__classify__dual                 : False\n",
      "COMET INFO:     lr__classify__fit_intercept        : True\n",
      "COMET INFO:     lr__classify__intercept_scaling    : 1\n",
      "COMET INFO:     lr__classify__l1_ratio             : 1\n",
      "COMET INFO:     lr__classify__max_iter             : 100\n",
      "COMET INFO:     lr__classify__multi_class          : auto\n",
      "COMET INFO:     lr__classify__n_jobs               : 1\n",
      "COMET INFO:     lr__classify__penalty              : l2\n",
      "COMET INFO:     lr__classify__random_state         : 1\n",
      "COMET INFO:     lr__classify__solver               : lbfgs\n",
      "COMET INFO:     lr__classify__tol                  : 0.0001\n",
      "COMET INFO:     lr__classify__verbose              : 0\n",
      "COMET INFO:     lr__classify__warm_start           : False\n",
      "COMET INFO:     lr__memory                         : 1\n",
      "COMET INFO:     lr__steps                          : [('Count', CountVectorizer(ngram_range=(1, 2))), ('classify', LogisticRegression(C=1))]\n",
      "COMET INFO:     lr__verbose                        : False\n",
      "COMET INFO:     max_depth                          : 1\n",
      "COMET INFO:     max_features                       : auto\n",
      "COMET INFO:     max_iter                           : 100\n",
      "COMET INFO:     max_leaf_nodes                     : 1\n",
      "COMET INFO:     max_samples                        : 1\n",
      "COMET INFO:     metric                             : minkowski\n",
      "COMET INFO:     metric_params                      : 1\n",
      "COMET INFO:     min_impurity_decrease              : 0.0\n",
      "COMET INFO:     min_samples_leaf                   : 1\n",
      "COMET INFO:     min_samples_split                  : 2\n",
      "COMET INFO:     min_weight_fraction_leaf           : 0.0\n",
      "COMET INFO:     multi_class                        : auto\n",
      "COMET INFO:     n_estimators                       : 100\n",
      "COMET INFO:     n_jobs                             : 1\n",
      "COMET INFO:     n_neighbors                        : 3\n",
      "COMET INFO:     neg_label                          : 0\n",
      "COMET INFO:     oob_score                          : False\n",
      "COMET INFO:     p                                  : 2\n",
      "COMET INFO:     param_grid                         : {\"C\": [0.001, 0.01, 0.1, 1, 10]}\n",
      "COMET INFO:     passthrough                        : False\n",
      "COMET INFO:     penalty                            : l2\n",
      "COMET INFO:     pos_label                          : 1\n",
      "COMET INFO:     pre_dispatch                       : 2*n_jobs\n",
      "COMET INFO:     probability                        : False\n",
      "COMET INFO:     random_state                       : 1561\n",
      "COMET INFO:     refit                              : True\n",
      "COMET INFO:     return_train_score                 : False\n",
      "COMET INFO:     rf                                 : Pipeline(steps=[('Count', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('classify', RandomForestClassifier())])\n",
      "COMET INFO:     rf__Count                          : CountVectorizer(ngram_range=(1, 2))\n",
      "COMET INFO:     rf__Count__analyzer                : word\n",
      "COMET INFO:     rf__Count__binary                  : False\n",
      "COMET INFO:     rf__Count__decode_error            : strict\n",
      "COMET INFO:     rf__Count__dtype                   : <class 'numpy.int64'>\n",
      "COMET INFO:     rf__Count__encoding                : utf-8\n",
      "COMET INFO:     rf__Count__input                   : content\n",
      "COMET INFO:     rf__Count__lowercase               : True\n",
      "COMET INFO:     rf__Count__max_df                  : 1.0\n",
      "COMET INFO:     rf__Count__max_features            : 1\n",
      "COMET INFO:     rf__Count__min_df                  : 1\n",
      "COMET INFO:     rf__Count__ngram_range             : (1, 2)\n",
      "COMET INFO:     rf__Count__preprocessor            : 1\n",
      "COMET INFO:     rf__Count__stop_words              : 1\n",
      "COMET INFO:     rf__Count__strip_accents           : 1\n",
      "COMET INFO:     rf__Count__token_pattern           : (?u)\\b\\w\\w+\\b\n",
      "COMET INFO:     rf__Count__tokenizer               : 1\n",
      "COMET INFO:     rf__Count__vocabulary              : 1\n",
      "COMET INFO:     scoring                            : f1_weighted\n",
      "COMET INFO:     shrinking                          : True\n",
      "COMET INFO:     solver                             : lbfgs\n",
      "COMET INFO:     sparse_output                      : False\n",
      "COMET INFO:     splitter                           : best\n",
      "COMET INFO:     tol                                : 0.0001\n",
      "COMET INFO:     verbose                            : False\n",
      "COMET INFO:     warm_start                         : False\n",
      "COMET INFO:     weights                            : uniform\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-info          : 1\n",
      "COMET INFO:     conda-specification : 1\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     notebook            : 1\n",
      "COMET INFO:     source_code         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET ERROR: Error sending a notification, make sure you have opted-in for notifications\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.53 s (started: 2022-06-23 16:56:03 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# End experiment\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"https://www.comet.ml/tumishang/global-warming-climate-change-sentiment-analysis/83a4c55ae1f44ed5abfea690ade303b7\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1b6a23ff520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-23 16:56:12 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# Display results on comet page\n",
    "experiment.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.03 s (started: 2022-06-23 16:56:36 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# Creating the unseen set, so that we can post to Kaggle and recieve a score based on the performance\n",
    "x_unseen = test['processed']\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    {'tweetid': test['tweetid'],\n",
    "     'sentiment': clf.predict(x_unseen)\n",
    "    })\n",
    "\n",
    "# save DataFrame to csv file for submission\n",
    "submission.to_csv(\"Submission_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
